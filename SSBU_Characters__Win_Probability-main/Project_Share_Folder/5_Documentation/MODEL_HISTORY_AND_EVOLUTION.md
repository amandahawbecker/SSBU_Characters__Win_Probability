# Model History and Evolution - Complete Timeline

## üìä Project Model Evolution

---

## **Phase 1: Regression Models** (Initial Approach)

### **Models Used:**

1. ‚úÖ **Linear Regression** (`LinearRegression` from sklearn)
2. ‚úÖ **Random Forest Regressor** (`RandomForestRegressor` from sklearn)
3. ‚úÖ **Gradient Boosting Regressor** (`GradientBoostingRegressor` from sklearn)

### **File:** `build_matchup_model.py`

### **Results:**

| Model | R¬≤ Score | RMSE | MAE |
|-------|----------|------|-----|
| **Linear Regression** | 0.0074 (0.74%) | 0.1694 | 0.1309 |
| **Random Forest** | **0.0636 (6.36%)** | **0.1645** | **0.1293** |
| **Gradient Boosting** | -0.0182 (-1.82%) | 0.1715 | 0.1347 |

**Best Model:** Random Forest Regressor
**Verdict:** Poor performance - only 6.36% variance explained ‚ùå

---

## **Phase 2: Classification Models** (Better Approach!)

### **Models Used:**

1. ‚úÖ **Logistic Regression** (`LogisticRegression` from sklearn)
2. ‚úÖ **Random Forest Classifier** (`RandomForestClassifier` from sklearn)
3. ‚úÖ **Gradient Boosting Classifier** (`GradientBoostingClassifier` from sklearn)

### **Files:** 
- `build_matchup_classifier.py` - Initial classification
- `improve_classifier.py` - Improved with class weights
- `build_enhanced_classifier.py` - Added technical parameters

### **Results:**

#### **Binary Classification (Who Wins?):**
- **Random Forest:** 85.48% accuracy ‚úÖ
- **Gradient Boosting:** Similar performance
- **Logistic Regression:** Lower accuracy

#### **Three-Class Classification (Advantaged/Even/Disadvantaged):**
- **Random Forest:** 73.92% accuracy ‚úÖ

**Best Model:** Random Forest Classifier
**Verdict:** Much better! ‚úÖ

---

## ‚ùå **XGBoost: NOT Actually Used**

### **Status:** Mentioned but Never Implemented

**Where XGBoost Was Mentioned:**
- `DEEP_LEARNING_RECOMMENDATION.md` - As an alternative to deep learning
- `PROJECT_RECOMMENDATIONS.md` - In recommendations
- `SUMMARY.md` - In suggested models

**Why It Wasn't Used:**
- The project used **Gradient Boosting** from scikit-learn instead
- XGBoost requires separate installation (`pip install xgboost`)
- Random Forest performed well enough (85% accuracy)
- No need for additional optimization

**Note:** XGBoost is similar to Gradient Boosting but more optimized. The project used scikit-learn's Gradient Boosting, which is built-in and doesn't require extra dependencies.

---

## üìã **Complete Model Timeline**

### **1. Initial Regression Attempt** (Phase 1)
```
Linear Regression ‚Üí R¬≤ = 0.74% ‚ùå
Random Forest Regressor ‚Üí R¬≤ = 6.36% ‚ùå
Gradient Boosting Regressor ‚Üí R¬≤ = -1.82% ‚ùå

Result: Regression didn't work well
```

### **2. Switch to Classification** (Phase 2)
```
Logistic Regression ‚Üí Lower accuracy
Random Forest Classifier ‚Üí 85.48% accuracy ‚úÖ WINNER!
Gradient Boosting Classifier ‚Üí Similar to RF

Result: Classification works much better!
```

### **3. Model Improvements**
```
- Added class weights (handle imbalance)
- Added technical parameters (18 features total)
- Improved to 84% accuracy with balanced classes
```

---

## üéØ **Final Model Stack**

### **What Was Actually Implemented:**

1. ‚úÖ **Random Forest Classifier** (Primary Model)
   - 100 trees
   - Max depth: 10
   - Class weights: 'balanced'
   - 84% accuracy

2. ‚úÖ **Gradient Boosting Classifier** (Alternative)
   - Used for comparison
   - Similar performance to RF

3. ‚úÖ **Logistic Regression** (Baseline)
   - Simple baseline model
   - Lower performance

### **What Was Mentioned But Not Used:**

- ‚ùå **XGBoost** - Only in recommendations
- ‚ùå **Neural Networks/Deep Learning** - Analyzed but rejected
- ‚ùå **Support Vector Machines** - Not tried

---

## üìä **Model Comparison Summary**

### **Regression vs Classification:**

| Approach | Best Model | Performance | Status |
|----------|-----------|-------------|--------|
| **Regression** | Random Forest | R¬≤ = 6.36% | ‚ùå Poor |
| **Classification** | Random Forest | 84% accuracy | ‚úÖ Good |

### **Classification Models:**

| Model | Accuracy | Notes |
|-------|----------|-------|
| **Random Forest** | **84-85%** | ‚úÖ Best performer |
| **Gradient Boosting** | ~83% | Similar to RF |
| **Logistic Regression** | ~75% | Baseline |

---

## ‚úÖ **Answer to Your Question**

**Q: "The project used linear regression, random forest, and xgboost first right?"**

**Answer:**
- ‚úÖ **Linear Regression** - YES, used in Phase 1 (regression)
- ‚úÖ **Random Forest** - YES, used in both Phase 1 (regression) and Phase 2 (classification)
- ‚ùå **XGBoost** - NO, was only mentioned in recommendations, never actually implemented

**What Was Actually Used:**
- Linear Regression ‚úÖ
- Random Forest ‚úÖ
- **Gradient Boosting** ‚úÖ (NOT XGBoost)

**Gradient Boosting vs XGBoost:**
- Similar algorithms (both are gradient boosting)
- Gradient Boosting = scikit-learn's version (built-in)
- XGBoost = separate library (more optimized, requires installation)
- Project used scikit-learn's Gradient Boosting, not XGBoost

---

## üîÑ **Summary**

**Initial Models (Phase 1 - Regression):**
1. Linear Regression
2. Random Forest Regressor
3. Gradient Boosting Regressor

**Later Models (Phase 2 - Classification):**
1. Logistic Regression
2. Random Forest Classifier ‚≠ê **WINNER**
3. Gradient Boosting Classifier

**XGBoost:** Mentioned but not implemented

